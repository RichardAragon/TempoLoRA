# ğŸµ TempoLoRA: Rhythm-Aware Finetuning for Video Diffusion

**TempoLoRA** is a lightweight finetuning framework for video diffusion models (e.g. **WAN 2.2**) that injects **temporal rhythm awareness** using **LoRA adapters** plus a **spectral regularization loss**.
The goal: teach models to *move with natural tempo* â€” reducing flicker, stabilizing motion, and aligning generated videos with realistic human & environmental rhythms.

---

## âœ¨ Features

* ğŸ•’ **Temporal-only LoRA** â†’ finetune just the time/attention modules; fast and VRAM-friendly.
* ğŸš **Spectral loss** â†’ penalizes high-frequency jitter, boosts natural motion bands (0.5â€“6 Hz).
* ğŸµ **Hidden soundtrack alignment** â†’ models learn to carry their own rhythm.
* âš¡ **Drop-in training cell** â†’ no architecture surgery; one Python script/Colab cell does it.
* ğŸ›  **WAN 2.2 ready** â†’ built for [Wan 2.2 TI2V and A14B Diffusers pipelines](https://huggingface.co/Wan-AI).

---

## ğŸš€ Installation

```bash
git clone https://github.com/yourname/TempoLoRA.git
cd TempoLoRA

# (recommended) create a new conda or venv
conda create -n tempolora python=3.10
conda activate tempolora

pip install -U torch torchvision --index-url https://download.pytorch.org/whl/cu124
pip install -U git+https://github.com/huggingface/diffusers.git
pip install transformers accelerate safetensors decord einops scipy
```

---

## ğŸ“‚ Dataset

TempoLoRA expects a directory of short videos + prompts file:

```
wan22_data/
  â”œâ”€â”€ clip1.mp4
  â”œâ”€â”€ clip2.mp4
  â””â”€â”€ prompts.txt
```

**prompts.txt** format:

```
clip1.mp4|a cat running across a field
clip2.mp4|a drone shot of waves crashing on rocks
```

If no data is found, the training script synthesizes a toy dataset (moving square) so you can test the loop.

---

## ğŸ§‘â€ğŸ’» Usage

Run the training script (or Colab cell):

```bash
python train_tempolora.py
```

Key outputs:

* `wan22_lora_out/wan22_temporal_lora.safetensors` â†’ LoRA checkpoint (ComfyUI / Diffusers compatible).
* `wan22_lora_out/sample_after_lora.mp4` â†’ quick sample video after training.

---

## âš™ï¸ Config (train\_tempolora.py)

You can edit parameters inside the config dataclass:

```python
MODEL_ID      = "Wan-AI/Wan2.2-TI2V-5B-Diffusers"  # or A14B
NUM_FRAMES    = 49
HEIGHT, WIDTH = 576, 1024
LORA_RANK     = 16
SPEC_CUTOFF_HZ = 6.0   # max natural frequency
SPEC_WEIGHT    = 0.05  # strength of spectral loss
TV_WEIGHT      = 0.01  # temporal smoothness
MAX_STEPS      = 800   # typical quick run
```

---

## ğŸ“Š Results

Baseline WAN 2.2 vs. TempoLoRA finetuned:

* âœ… Reduced temporal flicker on faces & textures.
* âœ… Smoother walking/camera pans.
* âœ… Emergent â€œtempo knobâ€ at inference: guiding frequency alters motion style.

---

## ğŸ”¬ Research Insight

Video diffusion models naturally fall into **latent rhythmic attractors** during training. TempoLoRA exposes and guides these attractors with a simple spectral loss. This is the first step toward **unifying video & audio diffusion** â€” teaching models to *generate with rhythm*.

---

## ğŸ–¼ Example (after 1k steps)

| Baseline WAN 2.2       | TempoLoRA               |
| ---------------------- | ----------------------- |
| ![](docs/baseline.gif) | ![](docs/tempolora.gif) |

---

## ğŸ¤ Acknowledgements

* Built on [WAN 2.2](https://huggingface.co/Wan-AI) (Diffusers).
* Inspired by rhythm analysis in biological motion and neuroscience.
* LoRA mechanics adapted from Hugging Face + PEFT community.

---

## ğŸ“œ License

MIT License (open-source, attribution appreciated).

